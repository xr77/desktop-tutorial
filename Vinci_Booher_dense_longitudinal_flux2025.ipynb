{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xr77/desktop-tutorial/blob/master/Vinci_Booher_dense_longitudinal_flux2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dense Longitudinal Neuroimaging: Temporally Precise Modeling of Brain Change Trajectories**\n",
        "\n",
        "---\n",
        "https://bit.ly/dense-longitudinal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kL_kbf3rLDgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 0**: Before we start, let's prepare the data and compute environment."
      ],
      "metadata": {
        "id": "KcQ4J5hPRilS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tutorial data from OSF, including functional and motion data for 12 visits from 1 participant.\n",
        "# This will take 5-7 minutes, depending on download speed.\n",
        "!wget https://files.osf.io/v1/resources/qz6dp/providers/osfstorage/?zip= -O /content/osf_qz6dp.zip\n",
        "!unzip -o \"/content/osf_qz6dp.zip\" -d \"/content/dense_longitudinal_data\"\n",
        "print(f\"âœ… Downloaded to /content/dense_longitudinal_data\")"
      ],
      "metadata": {
        "id": "7BUPPmaKe9uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install GLMsingle and nilearn.\n",
        "!pip install git+https://github.com/cvnlab/GLMsingle.git\n",
        "!pip install nilearn\n",
        "\n",
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import requests\n",
        "import glob\n",
        "import os\n",
        "from os.path import join, exists\n",
        "from IPython.display import YouTubeVideo\n",
        "from IPython.display import Image\n",
        "from scipy import interpolate\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "import random\n",
        "\n",
        "from glmsingle.glmsingle import GLM_single"
      ],
      "metadata": {
        "id": "WcRnovJ0fTQH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define internal functions to be used later.\n",
        "\n",
        "# Define an internal function that will convert our _events.tsv file to a sparse design matrix.\n",
        "def tsv_to_design(events_path, T, tr, use_duration=False, rounding='rint'):\n",
        "    df = pd.read_csv(events_path, sep='\\t')\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    if 'onset' not in df.columns:\n",
        "        raise ValueError(f\"{events_path} missing 'onset'\")\n",
        "    df = df.dropna(subset=['onset']).sort_values('onset').copy()\n",
        "\n",
        "    on = df['onset'].astype(float).to_numpy()\n",
        "\n",
        "    if use_duration and 'duration' in df.columns:\n",
        "        dur = df['duration'].astype(float).fillna(0.0).to_numpy()\n",
        "        dur = np.clip(dur, 0, None)\n",
        "        widths = np.maximum(1, np.ceil(dur / tr).astype(int))\n",
        "        idx = np.floor(on / tr).astype(int)   # align to start TR for multi-TR blocks\n",
        "    else:\n",
        "        widths = np.ones_like(on, dtype=int)  # single-TR impulse\n",
        "        mapper = {'rint': np.rint, 'floor': np.floor, 'ceil': np.ceil}\n",
        "        idx = mapper.get(rounding, np.rint)(on / tr).astype(int)\n",
        "\n",
        "    idx = np.clip(idx, 0, T - 1)\n",
        "\n",
        "    M = np.zeros((T, len(on)), dtype=np.float32)\n",
        "    for j, r0 in enumerate(idx):\n",
        "        a, b = max(0, r0), min(T, r0 + widths[j])\n",
        "        if a < b:\n",
        "            M[a:b, j] = 1.0\n",
        "    return M\n",
        "\n",
        "# Define time series interpolation function.\n",
        "def t_series_interpolation(data, old_tr, new_tr, *, fill_value=\"extrapolate\", dtype=np.float32):\n",
        "    \"\"\"\n",
        "    Interpolate data from old_tr to new_tr.\n",
        "    data: 4D (X, Y, Z, T)\n",
        "    old_tr, new_tr: second\n",
        "    fill_value: 'extrapolate'\n",
        "    dtype: data type\n",
        "    \"\"\"\n",
        "    n_vol = data.shape[-1]\n",
        "    if n_vol < 2 or old_tr == new_tr: # cannot perform interpolation if T < 2\n",
        "        return data.astype(dtype, copy=True)\n",
        "\n",
        "    # create the old time series\n",
        "    old_t = np.arange(n_vol) * old_tr\n",
        "    t_end = (n_vol - 1) * old_tr\n",
        "\n",
        "    # create new time series\n",
        "    n_new = int(np.ceil(n_vol * old_tr / new_tr))\n",
        "    new_t = np.linspace(0.0, (n_new - 1) * new_tr, n_new)\n",
        "\n",
        "    # set the interpolation\n",
        "    f = interpolate.interp1d(\n",
        "        old_t, data, kind=\"linear\", axis=-1,\n",
        "        bounds_error=False, fill_value=fill_value, assume_sorted=True\n",
        "    )\n",
        "    out = f(new_t).astype(dtype, copy=False)\n",
        "    return out\n",
        "\n",
        "# Define function to plot ROI results across visits.\n",
        "def plot_roi_results(m, md, sd, se, roi_name):\n",
        "\n",
        "    \"\"\"Plot ROI results across visits - displays plot and saves to file.\"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "    xval = m.index+1  # Visit numbers\n",
        "\n",
        "    # Zero line\n",
        "    gray = [0.5, 0.5, 0.5]\n",
        "    ax.axhline(0, linestyle=':', color=gray)\n",
        "\n",
        "    # Plot faces data\n",
        "    # color definitions\n",
        "    fcolor = [0.94, 0.5, 0.5]    # Faces\n",
        "    ax.errorbar(xval, m['Faces'], se['Faces'], color=fcolor, linewidth=5, capsize=0)\n",
        "    ax.scatter(xval, m['Faces'], color=fcolor, s=300, edgecolors='white')\n",
        "    ax.axhline(y=m['Faces'].mean(), color=fcolor + [0.6], linewidth=1)\n",
        "\n",
        "    # Plot letters data\n",
        "    lcolor = [0, 0.5, 1]         # Letters\n",
        "    ax.errorbar(xval, m['Letters'], se['Letters'], color=lcolor, linewidth=5, capsize=0)\n",
        "    ax.scatter(xval, m['Letters'], color=lcolor, s=300, edgecolors='white')\n",
        "    ax.axhline(y=m['Letters'].mean(), color=lcolor + [0.6], linewidth=1)\n",
        "\n",
        "    # Axis settings\n",
        "    ax.set_xlim(0.5, 12.5)\n",
        "    ax.set_xticks(np.arange(1,13))\n",
        "    ax.set_xlabel('Month', fontsize=16)\n",
        "    ax.set_ylabel('Beta Value', fontsize=16)\n",
        "    ax.set_title(roi_name, fontsize=18)\n",
        "\n",
        "    # Legend\n",
        "    ax.legend(['', 'Faces', '', 'Letters', ''], loc='upper right', frameon=False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save\n",
        "    save_path = os.path.join(outdir, f'plot_{roi_name}.png')\n",
        "    fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
        "    print(f\"Plot saved: {save_path}\")\n",
        "\n",
        "# Define function to align beta and ROI dimensions (e.g., cropping in Z).\n",
        "def align_xyz(beta3d, mask3d, tag=\"\"):\n",
        "    if beta3d.shape[:2] != mask3d.shape[:2]:\n",
        "        raise ValueError(f\"[{tag}] X/Y mismatch: beta {beta3d.shape} vs ROI {mask3d.shape}\")\n",
        "    if beta3d.shape[2] != mask3d.shape[2]:\n",
        "        z_new = min(beta3d.shape[2], mask3d.shape[2])\n",
        "        print(f\"âš ï¸ [{tag}] Cropping Z to {z_new} (beta Z={beta3d.shape[2]}, ROI Z={mask3d.shape[2]})\")\n",
        "        beta3d = beta3d[:, :, :z_new]\n",
        "        mask3d = mask3d[:, :, :z_new]\n",
        "    return beta3d, mask3d\n",
        "\n",
        "# Define simple helper function help with reading rootdir.\n",
        "def visit_dir(v):\n",
        "    \"\"\"Return full path to a visit directory\"\"\"\n",
        "    return Path(rootdir) / \"function\" / f\"sub-03{v}\"\n",
        "\n",
        "# Define configuration function to handle different indexing between Matlab\n",
        "# (i.e., 1 indexing) and Python (i.e., 0 indexing).\n",
        "def load_modelmd(filepath):\n",
        "    \"\"\"Load and reshape beta map from .mat file\"\"\"\n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        data = np.array(f['modelmd'][()])\n",
        "\n",
        "        # Fix dimension order from MATLAB to (X,Y,Z,T)\n",
        "        expected_shape = (58, 73, 57, 208) # this is specific to the sample dataset\n",
        "        if data.ndim == 3:\n",
        "            data = np.transpose(data, (2, 1, 0))  # (Y,Z,T) â†’ (Z,Y,T) â†’ needs pad\n",
        "            data = np.pad(data, [\n",
        "                (0, expected_shape[1] - data.shape[0]),\n",
        "                (0, expected_shape[2] - data.shape[1]),\n",
        "                (0, 0)\n",
        "            ])\n",
        "            data = np.expand_dims(data, axis=0)  # add X dimension\n",
        "        elif data.ndim == 4:\n",
        "            data = np.transpose(data, (3, 2, 1, 0))  # (T,Z,Y,X) â†’ (X,Y,Z,T)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Define a function to load in design information outpus from GLMsingle and\n",
        "# configure for Python.\n",
        "def load_designinfo(filepath):\n",
        "    \"\"\"Load design info: stimorder, tr, stimdur\"\"\"\n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        return {\n",
        "            'stimorder': np.array(f['stimorder'][()]).flatten(),\n",
        "            'tr': float(f['tr'][()][0, 0]),\n",
        "            'stimdur': float(f['stimdur'][()][0, 0])\n",
        "        }"
      ],
      "metadata": {
        "id": "v0OruS5QMUZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some background while we wait..."
      ],
      "metadata": {
        "id": "x3EddiDFffwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dense longitudinal neuroimaging** is a type of precision neuroimaging where the precision is applied to the sampling of the change trajectory -- an approach that is particularly relevant for studies of development and learning."
      ],
      "metadata": {
        "id": "rc2s1vCvgwSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url=\"https://raw.githubusercontent.com/Learning-and-Neurodevelopment-Lab/dense-longitudinal-tutorial/refs/heads/main/fig-1.png\", width=800)"
      ],
      "metadata": {
        "id": "-PUh0iMgB8vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url=\"https://raw.githubusercontent.com/Learning-and-Neurodevelopment-Lab/dense-longitudinal-tutorial/refs/heads/main/fig-2.png\", width=800)"
      ],
      "metadata": {
        "id": "kzGnct8eFXpj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial analyzes dense longitudinal neuroimaging data to **quantify changes in brain activation within the visual word form area (VWFA) and the fusiform face area (FFA) throughout first grade**. More specifically, we will analyze task-based fMRI data from one child's brain sampled every month throughout their first grade year."
      ],
      "metadata": {
        "id": "X44k1p31-A-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis approach is **an individualized ROI analysis**: Identify FFA AND VWFA using a standard localizer. Then, use a different localizer to extract estimates of brain responses from those ROIs every month from the beginning to the end of first grade."
      ],
      "metadata": {
        "id": "jPWBbelcYPkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1**: Collect dense longitudinal data."
      ],
      "metadata": {
        "id": "5AjSgpT1krFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Acquire dense longitudinal neuroimaging data that we can use to estimate changes in brain responses within the VWFA and FFA throughout the first grade.  "
      ],
      "metadata": {
        "id": "2QEh2WyBDE4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will**: Discuss (1) the experimental paradigm, (2) practical considerations related to retention, and (3) an evaluation of motion for dense longitudinal neuroimaging data."
      ],
      "metadata": {
        "id": "2dZJH_QlFQyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) THE EXPERIMENTAL PARADIGM."
      ],
      "metadata": {
        "id": "SJkX-fQPKtx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use our ([SS](https://github.com/Learning-and-Neurodevelopment-Lab/Sesame_Street_Cohort1)) paradigm that **presents participants with images from Sesame Street**. Only one image is presented per trial so that each trial can be labeled for different object categories, depending on the experimenter's needs. There are approximately 208 unique images presented at each visit."
      ],
      "metadata": {
        "id": "r-ETL1XkaETW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878a02b3"
      },
      "source": [
        "YouTubeVideo(\"Tb37rSZghNQ\", embed=True, width=640, height=320)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this tutorial, **we're labeling each image (i.e., trial) for the presence or absence of faces and letters** so that we can use the data to measure face- and letter-selectivity within the VWFA and FFA."
      ],
      "metadata": {
        "id": "ij7_RKjZPX5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) PRACTICAL CONSIDERATIONS RELATED TO RETENTION."
      ],
      "metadata": {
        "id": "IBn1LcamZ5ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url=\"https://raw.githubusercontent.com/Learning-and-Neurodevelopment-Lab/dense-longitudinal-tutorial/refs/heads/main/fig-3.png\", width=400)"
      ],
      "metadata": {
        "id": "UDaosOMYe22o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There were 12 visits that occurred monthly, starting the month before first grade and ending the month after first grade."
      ],
      "metadata": {
        "id": "yIAMYunW9xbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) AN EVALUATION OF MOTION FOR DENSE LONGITUDINAL NEUROIMAGING DATA.\n",
        "\n",
        "# Set basic variables.\n",
        "subid = 'sub-03'\n",
        "visits = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
        "rootdir = '/content/dense_longitudinal_data/'\n",
        "\n",
        "# Define the folder where motion data is located\n",
        "data_folder = os.path.join(rootdir, \"motion\")\n",
        "\n",
        "# Initialize lists to store the mean and standard deviation for each month\n",
        "means = []\n",
        "stds = []\n",
        "\n",
        "# Loop through the 12 months (visits)\n",
        "for month in range(1, 13):\n",
        "    subject_month_id = f\"03{month:02d}\"\n",
        "\n",
        "    # This list will hold the mean value from each of the 4 runs\n",
        "    run_means = []\n",
        "\n",
        "    # Loop through the 4 runs for the current month\n",
        "    for run in range(1, 5):\n",
        "\n",
        "        file_name = f\"sub-{subject_month_id}_task-ss_run-0{run}_desc-confounds_timeseries.tsv\"\n",
        "        file_path = os.path.join(data_folder, file_name)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path, sep='\\t')\n",
        "            if 'framewise_displacement' in df.columns and not df['framewise_displacement'].dropna().empty:\n",
        "                run_means.append(df['framewise_displacement'].mean())\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "\n",
        "    if run_means:\n",
        "        run_means_series = pd.Series(run_means)\n",
        "        means.append(run_means_series.mean())\n",
        "        stds.append(run_means_series.std())\n",
        "    else:\n",
        "        means.append(None)\n",
        "        stds.append(None)\n",
        "\n",
        "# Create the plot\n",
        "months = range(1, 13)\n",
        "plt.errorbar(months, means, yerr=stds, fmt='-o', capsize=5, label='Monthly Mean with Std Dev')\n",
        "\n",
        "# Calculate and plot the overall mean/std\n",
        "if pd.Series(means).notna().any():\n",
        "    grand_mean = pd.Series(means).mean()\n",
        "    grand_std = pd.Series(means).std()\n",
        "\n",
        "    plt.axhline(y=grand_mean, color='r', linestyle='--', label='Mean across 12 Months')\n",
        "    plt.axhspan(grand_mean - grand_std, grand_mean + grand_std,\n",
        "                color='r', alpha=0.2, label='SD across 12 Months')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Motion (framewise displacement in mm)\")\n",
        "plt.title(\"Subject 03\")\n",
        "plt.xticks(months)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 3)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sq7qXfWpe3P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2**: Estimate responses for face and letter images at all visits."
      ],
      "metadata": {
        "id": "yiUEU0bEB1L4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Calculate beta-weights to estimate brain responses to face and letter images at all visits.   "
      ],
      "metadata": {
        "id": "0lxNEAWPqWLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will**: (1) Prepare SS design matrix â†’ (2) Prepare the SS functional data â†’ (3) Run GLMsingle â†’ (4) Visualize data quality outputs."
      ],
      "metadata": {
        "id": "9gT8EyvXt6bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use **GLMsingle to get trial-specific beta-weights**, meaning that we get a measure of activation to each image (i.e., trial). Using GLMsingle with the SS design allows analyses beyond face- and letter responses. We can select any category of interest, for example, and then relabel the trials for our new category of interest."
      ],
      "metadata": {
        "id": "WtGL-kUdGQ8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Prepare SS design matrix."
      ],
      "metadata": {
        "id": "wYvoRP1_buDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables for GLMsingle.\n",
        "\n",
        "tr = 1.0        # expected length of tr, required input to GLMsingle\n",
        "nvol = 294      # expected number of volumes when length of TR is set to 1.0\n",
        "stimdur  = 3    # stimulus duration, required input to GLMsingle"
      ],
      "metadata": {
        "id": "-LA-B3tIcDfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CONVERT BIDS-READY _EVENTS.TSV TO A GLMSINGLE-READY DESIGN MATRIX.\n",
        "\n",
        "# To estimate brain responses to single images, the design matrix should contain\n",
        "# one predicter for each unique image. Design Matrix Shape: (T, C).\n",
        "# T: Number of TRs. C: Number of unique images shown.\n",
        "\n",
        "# Define _event.tsv file locations.\n",
        "derivatives_path = f'{rootdir}/function/sub-0312/func'\n",
        "\n",
        "# find events files (one per run)\n",
        "events_files = sorted(glob.glob(os.path.join(derivatives_path, f'*task-ss*_events.tsv')))\n",
        "\n",
        "design = []\n",
        "design = [tsv_to_design(evt, nvol, tr, use_duration=False, rounding='rint')\n",
        "          for evt in events_files]\n",
        "\n",
        "print(f\"Found {len(events_files)} runs.\")\n",
        "print(\"run 1 design shape:\", design[0].shape)\n",
        "print(\"run 2 design shape:\", design[1].shape)"
      ],
      "metadata": {
        "id": "Ad1JggoA5-bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot first run (imshow)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(design[0], aspect='auto', origin='lower', interpolation='nearest')\n",
        "plt.title(f\"design (run 1)  T={design[0].shape[0]}, trials={design[0].shape[1]}\")\n",
        "plt.xlabel('Condition (i.e., image)'); plt.ylabel('TR (after interpolation)')\n",
        "plt.colorbar(label='0/1')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zorPO8UCbHUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### (2) Prepare the SS functional data."
      ],
      "metadata": {
        "id": "rf01EZTxtrSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that all of the data we are working with have already been minimally preprocessed using [fMRIprep](https://fmriprep.org/en/stable/), and this tutorial won't cover the preprocessing steps."
      ],
      "metadata": {
        "id": "BEZ_IR0MzZBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPORT SS FUNCTIONAL DATA.\n",
        "\n",
        "# selected func files\n",
        "datafiles = sorted(glob.glob(os.path.join(\n",
        "    derivatives_path, f'*task-ss*_space-T1w_desc-preproc_bold.nii.gz')))\n",
        "\n",
        "# load the data and convert to NumPy array\n",
        "data = [nib.load(f).get_fdata().astype(np.float32) for f in datafiles]\n",
        "\n",
        "# Get the number of volumes from the NumPy array.\n",
        "T = data[0].shape[-1]\n",
        "\n",
        "# Get the number of volumes in xyz from the NumPy array.\n",
        "xyz= data[0].shape[:3]\n",
        "# print(f'data shape: {data[1].shape}')  # expected (x, y, z, T)\n",
        "#print(\"len(data)  =\", len(data)  if isinstance(data,  (list,tuple)) else \"not a list\")\n",
        "\n",
        "print(f\"\\nSS fMRI data files were identified for {len(data)} runs:\")\n",
        "print(\"\\n\".join(f\"{i}. {os.path.basename(f)}\" for i, f in enumerate(datafiles)))\n",
        "print(f'\\nThe shape of the fMRI data set for each run is: {data[0].shape}')  # expected (x, y, z, T)"
      ],
      "metadata": {
        "id": "evkDQ6yOcZSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZE FUNCTIONAL DATA FROM EACH SS RUN.\n",
        "\n",
        "slice_idx = 40\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i in range(len(data)):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.imshow(data[i][:,:,slice_idx,0], cmap='gray')\n",
        "    plt.title(f'Example slice from run {i+1}', fontsize=14)\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lGKfbzqN8bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TIME SERIES INTERPOLATION.\n",
        "\n",
        "# Resample 4D fMRI time series (with shape `(X, Y, Z, T)`) from the original TR (`orig_tr`)\n",
        "# to a new TR (`tr`) using internally defined function (see the beginning of the tutorial!).\n",
        "\n",
        "orig_tr = 1.5\n",
        "data = [t_series_interpolation(d, orig_tr, tr) for d in data]\n",
        "\n",
        "# Print some relevant metadata\n",
        "print(f'Dimensions of the new run 1 time series after interpolation: {data[0].shape}')\n",
        "print(f'Dimensions of the new run 2 time series after interpolation: {data[1].shape}')\n",
        "print(f'\\nData has {len(data)} runs\\n')\n",
        "print(f'Shape of data from each run is: {data[0].shape}')\n",
        "print(f'XYZ dimensionality is: {data[0].shape[:3]} (one slice only)')\n",
        "print(f'N = {data[0].shape[3]} TRs per run')"
      ],
      "metadata": {
        "id": "5jTb7eOuzCCn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(3) Run GLMsingle."
      ],
      "metadata": {
        "id": "jOzbij77RGbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **every voxel**, GLMsingle:\n",
        "\n",
        "1.   Selects the best HRF function.\n",
        "2.   Applies GLMdenoise to remove noise.\n",
        "3.   Fractional ridge regression.\n",
        "\n",
        "Together, these three features to maximize data quality at the **individual voxel** and **individual trial** levels."
      ],
      "metadata": {
        "id": "mNlJG37Pq62U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set options for GLMsingle.\n",
        "opt = dict()\n",
        "opt['wantlibrary'] = 1 # yes, we want to use GLMsingles built-in HRF library\n",
        "opt['wantglmdenoise'] = 1 # yes, we want to apply GLMdenoise\n",
        "opt['wantfracridge'] = 1 # yes, we want to use Fractional Ridge Regression for single-trial beta estimates\n",
        "opt['wantfileoutputs'] = [1,1,1,1]\n",
        "opt['wantmemoryoutputs'] = [1,1,1,1]\n",
        "glmsingle_obj = GLM_single(opt)\n",
        "\n",
        "# Print out options for GLMsingle.\n",
        "print(\"GLMsingle hyperparameters:\")\n",
        "pprint(glmsingle_obj.params)"
      ],
      "metadata": {
        "id": "rXrMcuTJSjzo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do it! (But actually don't becaust it will take too long! Just load in some\n",
        "# pre-baked outputs for now.\n",
        "\n",
        "# Because we already have the data processed, the code below will import it.\n",
        "# However, the code below will also run GLMsingle, if the outputs don't already\n",
        "# exist.\n",
        "\n",
        "# Set output directories.\n",
        "outputdir_glmsingle = os.path.join(derivatives_path, \"func-ss-glmsingle\")\n",
        "os.makedirs(outputdir_glmsingle, exist_ok=True)\n",
        "print(\"GLMsingle outputs available at:\", outputdir_glmsingle)\n",
        "\n",
        "# Define expected GLMsingle output files\n",
        "typeA = join(outputdir_glmsingle,'TYPEA_ONOFF.npy')\n",
        "typeB = join(outputdir_glmsingle,'TYPEB_FITHRF.npy')\n",
        "typeC = join(outputdir_glmsingle,'TYPEC_FITHRF_GLMDENOISE.npy')\n",
        "typeD = join(outputdir_glmsingle,'TYPED_FITHRF_GLMDENOISE_RR.npy')\n",
        "\n",
        "if not exists(typeD):\n",
        "\n",
        "  # Run GLMsingle.\n",
        "  results_glmsingle = glmsingle_obj.fit(\n",
        "      design,           # List of design matrices (one per run)\n",
        "      data,             # List of 4D BOLD data arrays (X, Y, Z, T)\n",
        "      stimdur,          # Stimulus duration in seconds\n",
        "      tr,               # Repetition time (TR) in seconds = 1.0\n",
        "      outputdir=outputdir_glmsingle  # Where to save the model outputs (Aâ€“D)\n",
        "      )\n",
        "\n",
        "else:\n",
        "\n",
        "  # Load GLMsingle outputs.\n",
        "  typeA = np.load(typeA, allow_pickle=True).item()\n",
        "  typeB = np.load(typeB, allow_pickle=True).item()\n",
        "  typeC = np.load(typeC, allow_pickle=True).item()\n",
        "  typeD = np.load(typeD, allow_pickle=True).item()\n",
        "  results_glmsingle = {'typed': {'betasmd': typeD}}"
      ],
      "metadata": {
        "id": "X2ctDQ9WSsWi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(4) Visualize data quality outputs."
      ],
      "metadata": {
        "id": "BjC51yQX759v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLMsingle implements some important data quality steps that allow more precise estimates of individual brain responses than standard GLM approaches."
      ],
      "metadata": {
        "id": "D5kgL0jo5S-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the first proc functional run\n",
        "\n",
        "print(f\"data[0] shape: {data[0].shape}\")  # (X, Y, Z, T)\n",
        "\n",
        "# Extract spatial dimensions (X, Y, Z)\n",
        "xyz = data[0].shape[:3]\n",
        "\n",
        "# Compute the mean volume across time (TRs) from the first run\n",
        "meanvol = np.mean(data[0], axis=3)\n",
        "if len(data) > 1:\n",
        "    meanvol = 0.5 * (meanvol + np.mean(data[1], axis=3))  # Optional: blend first two runs\n",
        "\n",
        "# Generate a brain mask by thresholding low-intensity background\n",
        "thr = np.percentile(meanvol[meanvol > 0], 40) if np.any(meanvol > 0) else np.percentile(meanvol, 40)\n",
        "brainmask = meanvol > thr"
      ],
      "metadata": {
        "id": "IU1Wifvg75WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot quality assurance: betasmd.\n",
        "\n",
        "# betasmd: is the full set of single-trial beta weights (X x Y x Z x TRIALS). beta weights are arranged in chronological order.\n",
        "\n",
        "betas = typeD['betasmd']\n",
        "\n",
        "# For beta maps, average over all trials (axis 3)\n",
        "if betas.ndim == 4:\n",
        "    # (X, Y, Z, TRIAL)\n",
        "    plot_data = np.nanmean(betas, axis=3).astype(float)\n",
        "elif betas.ndim == 3:\n",
        "    plot_data = betas.astype(float)\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected betas shape: {betas.shape}\")\n",
        "\n",
        "slice_idx = 20\n",
        "sl = plot_data[:, 5:-5, slice_idx].copy()\n",
        "sl[~brainmask[:, 5:-5, slice_idx]] = np.nan\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(sl, cmap='RdBu_r', vmin=-5, vmax=5, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title('average GLM betas (runs 1-2)')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HozllrjvgBOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot quality assurance: R2.\n",
        "\n",
        "# R2: model accuracy expressed in terms of R^2 (percentage).\n",
        "\n",
        "# For other GLMsingle outputs, reshape to volume\n",
        "R2 = typeD['R2']\n",
        "\n",
        "if R2.ndim == 4:\n",
        "    plot_data = np.nanmean(R2, axis=3).astype(float)\n",
        "elif R2.ndim == 3:\n",
        "    plot_data = R2.astype(float)\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected R2 shape: {R2.shape}\")\n",
        " # Cut slice (remove 5 voxels on top and bottom in Y to avoid edge effects)\n",
        "\n",
        "slice_idx = 20;\n",
        "sl = plot_data[:, 5:-5, slice_idx].copy()\n",
        "mask_sl = brainmask[:, 5:-5, slice_idx]\n",
        "sl[~mask_sl] = np.nan\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(sl, cmap='hot', vmin=0, vmax=85, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title('RÂ² (mean across runs)')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VIF7QiT3eu95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot quality assurance: HRFindex.\n",
        "\n",
        "# HRFindex: is the 1-index of the best fit HRF. HRFs can be recovered with getcanonicalHRFlibrary(stimdur,tr).\n",
        "\n",
        "# For other GLMsingle outputs, reshape to volume\n",
        "R2 = typeD['HRFindex']\n",
        "\n",
        "if R2.ndim == 4:\n",
        "    plot_data = np.nanmean(R2, axis=3).astype(float)\n",
        "elif R2.ndim == 3:\n",
        "    plot_data = R2.astype(float)\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected R2 shape: {R2.shape}\")\n",
        "# Cut slice (remove 5 voxels on top and bottom in Y to  avoid edge effects)\n",
        "slice_idx = 20 # Change slc to visualize different slices.\n",
        "sl = plot_data[:, 5:-5, slice_idx].copy()\n",
        "sl[~brainmask[:, 5:-5, slice_idx]] = np.nan\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(sl, cmap='jet', vmin=0, vmax=20, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title('HRF index')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lAwBOZm8ew8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot quality assurance: FRACvalue.\n",
        "\n",
        "# FRACvalue: is the fractional ridge regression regularization level chosen for each voxel. Values closer to 1 mean less regularization.\n",
        "\n",
        "FRAC = typeD['FRACvalue']\n",
        "\n",
        "# Start plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "if FRAC.ndim == 4:\n",
        "    plot_data = np.nanmean(FRAC, axis=3).astype(float)\n",
        "elif FRAC.ndim == 3:\n",
        "    plot_data = FRAC.astype(float)\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected FRACvalue shape: {FRAC.shape}\")\n",
        "\n",
        "# Cut slice (remove 5 voxels on top and bottom in Y to avoid edge effects)\n",
        "slice_idx = 20 # Change slc to visualize different slices.\n",
        "sl = plot_data[:, 5:-5, slice_idx].copy()\n",
        "sl[~brainmask[:, 5:-5, slice_idx]] = np.nan\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(sl, cmap='copper', vmin=0, vmax=1, origin='lower')\n",
        "plt.colorbar()\n",
        "plt.title('Voxel fraction (mean across runs)')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kdnGFTD0eypa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3**: Identify FFA and VWFA regions of interest (ROIs)."
      ],
      "metadata": {
        "id": "KzSW8Sk1kmlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Visualize the ROIs that we will be analyzing.  \n",
        "**We will**: (1) Import previously-defined ROI masks â†’ (2) Import the functional data â†’ (3) Visualize the ROIs on our functional data!"
      ],
      "metadata": {
        "id": "8ffCDite576k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Participants also completed a standard localizer procedure ([fLoc](https://github.com/Learning-and-Neurodevelopment-Lab/fLoc)) that has been **designed to identify person-specific category selective responses in ventral-temporal cortex**. We will use fLoc data to identify ROIs consistent with VWFA and FFA."
      ],
      "metadata": {
        "id": "DyLNX3YPZVDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YouTubeVideo(\"_IQ0QDOukoc\", embed=True, width=640, height=320)"
      ],
      "metadata": {
        "id": "VyABeNSzTmq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying it to the final visit will increase the likelihood that we will find the VWFA ROI in this individual child because we are more likely to find word-selectivity after first grade (at the final visit) than before first grade (at the first visit)."
      ],
      "metadata": {
        "id": "P3Wm2mt6C2Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### (1) IMPORT PREVIOUSLY-DEFINED ROI MASKS.\n",
        "\n",
        "# We used the fLoc experiment to identify an area in the right ventral temporal\n",
        "# cortex (rvtc) that responded selectively to faces\n",
        "# (faces>(places+letters+digits)) and another area in left ventral temporal\n",
        "# cortex (lvtc) that responded selectively letters\n",
        "# (letters>(faces+places+digits)), t>1.96.\n",
        "\n",
        "# Define the path to the **roi masks**.\n",
        "roi_dir = os.path.join(rootdir, \"function\", f\"{subid}12\", \"func\", \"func-floc-glmsingle\", \"contrasts-floc\", \"ROIs\")\n",
        "\n",
        "# Find all roi masks for face responsive areas, right hemisphere.\n",
        "face_roi_path   = [f for f in os.listdir(roi_dir) if \"faces\" in f and f.endswith(\".nii.gz\")][0]\n",
        "\n",
        "# Find all roi masks for letter responsive areas, left hemisphere.\n",
        "letter_roi_path = [f for f in os.listdir(roi_dir) if \"letters\" in f and f.endswith(\".nii.gz\")][0]\n",
        "\n",
        "# Load ROI masks.\n",
        "faces_roi   = nib.load(os.path.join(roi_dir, face_roi_path)).get_fdata()\n",
        "letters_roi = nib.load(os.path.join(roi_dir, letter_roi_path)).get_fdata()\n",
        "\n",
        "# Print filepaths.\n",
        "print(\"\\nDirectory where the ROI masks are located:\\n\", roi_dir)\n",
        "print(\"\\nList of ROI masks found:\")\n",
        "for f in os.listdir(roi_dir):\n",
        "    print(f)\n",
        "\n",
        "# Print summary of how many voxels within that ROI.\n",
        "print(f'\\nThere are {np.sum(faces_roi)} voxels in the included FFA ROI')\n",
        "print(f'There are {np.sum(letters_roi)} voxels in the included VWFA ROI')"
      ],
      "metadata": {
        "id": "Esh6vu-fLSCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (2) IMPORT THE FUNCTIONAL SS DATA FOR RUN 1 OF VISIT 12.\n",
        "\n",
        "# We're only going to import one run because we're just using the functional\n",
        "# data for visualization.\n",
        "\n",
        "# Define directory where the preprocessed functional data can be found.\n",
        "func_dir = os.path.join(rootdir, \"function\", f\"{subid}12\", \"func\")\n",
        "\n",
        "# Define file path to the preprocessed run 1 functional SS data.\n",
        "func_template_path = [f for f in os.listdir(func_dir) if \"task-ss_run-01_space-T1w_desc-preproc\" in f and f.endswith(\"_bold.nii.gz\")][0]\n",
        "\n",
        "# Load run 1 preprocessed functional data.\n",
        "fmri_data = nib.load(os.path.join(func_dir, func_template_path)).get_fdata()[:,:,:,0]\n",
        "\n",
        "# Print filepaths.\n",
        "print(\"\\nDirectory where visit 12 functional data are located:\\n\", func_dir)\n",
        "print(\"\\nFilename for functional SS data for run 1 of visit 12:\\n\", func_template_path)"
      ],
      "metadata": {
        "id": "KcUV3jPhn9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### (3) VISUALIZE FLOC ROI MASKS ON SS FUNCTIONAL DATA.\n",
        "\n",
        "# Set slice_idx to visualize slices of interest. TIP: Change 20 to other\n",
        "# numbers, e.g., 21, and then rerun this block to surf through the axial slices!\n",
        "slice_idx = 21\n",
        "\n",
        "# Set the figure size.\n",
        "plt.figure(figsize=(5, 4))\n",
        "\n",
        "# Select the fmri data for the slice of interest.\n",
        "bg = fmri_data[:,:,slice_idx].T\n",
        "\n",
        "# Plot the fmri data of the selected slice using a gray colormap.\n",
        "plt.imshow(bg, cmap='gray');\n",
        "\n",
        "# Select the ROI face mask data for the slice of interest.\n",
        "face_roi = faces_roi[:,:,slice_idx].T\n",
        "\n",
        "# Plot the ROI face mask in \"autumn\" to display in red.\n",
        "plt.imshow(np.where(face_roi>0, 1, np.nan), cmap='autumn', alpha=0.95);\n",
        "\n",
        "# Select the ROI letter mask data for the slice of interest.\n",
        "letter_roi = letters_roi[:,:,slice_idx].T\n",
        "\n",
        "# Plot the ROI letter mask in \"cool\" to display in cyan.\n",
        "plt.imshow(np.where(letter_roi>0, 1, np.nan), cmap='cool', alpha=0.95);\n",
        "\n",
        "# Clean up the disply by removing the axes labels.\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "Rwt9gSmPjlOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4**: Extract brain response estimates for face and letter images from voxels in the FFA and VWFA."
      ],
      "metadata": {
        "id": "a9kkoS36LScg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Calculate mean and variance for brain responses to face and letter images in the VWFA and FFA ROIs."
      ],
      "metadata": {
        "id": "vGVxo4WRI8LR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will**: (1) Load SS beta-weights for all runs and visits. â†’ (2) Reload the fLoc ROI masks. â†’ (3) Compute statistics for plotting."
      ],
      "metadata": {
        "id": "Ukzw0saMISuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## (1) Load beta-weights for all runs and visits.\n",
        "\n",
        "designs = []\n",
        "betas = []\n",
        "\n",
        "for v in visits:\n",
        "    print(f\"ðŸ”„ Loading visit {v}...\")\n",
        "    vp = visit_dir(v)\n",
        "    design_path = vp / 'DESIGNINFO.mat'\n",
        "    beta_path = vp / 'TYPED_FITHRF_GLMDENOISE_RR.mat'\n",
        "\n",
        "    if not design_path.exists() or not beta_path.exists():\n",
        "        print(f\"âŒ Missing files in visit {v}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        designs.append(load_designinfo(design_path))\n",
        "        betas.append(load_modelmd(beta_path))\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading visit {v}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- Final report ---\n",
        "if betas:\n",
        "    print(f\"\\nâœ… Successfully loaded {len(betas)} visits\")\n",
        "    print(f\"Beta shape: {betas[0].shape}\")\n",
        "    print(f\"Stimorder length: {len(designs[0]['stimorder'])}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No data loaded.\")"
      ],
      "metadata": {
        "id": "4zV1tHKg-5sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## (2) Reload the fLoc ROI masks.\n",
        "\n",
        "target_xyz = (58, 73, 57)  # e.g., known from previous data (58, 73, 57)\n",
        "\n",
        "# ---- Reload ROI masks ----\n",
        "roifiles = sorted([f for f in os.listdir(roi_dir) if f.endswith(\".nii.gz\") and \"nvox=0\" not in f])\n",
        "roi_masks, roi_names = [], []\n",
        "\n",
        "for f in roifiles:\n",
        "    img = nib.load(os.path.join(roi_dir, f))\n",
        "    # Optional: convert to canonical RAS+ orientation for consistency\n",
        "    img = nib.as_closest_canonical(img)\n",
        "    arr = np.asanyarray(img.dataobj)   # Expected shape: (58, 73, 57)\n",
        "    mask = arr > 0\n",
        "    assert mask.ndim == 3\n",
        "    roi_masks.append(mask)\n",
        "    roi_names.append(f.replace(\".nii.gz\",\"\"))\n",
        "\n",
        "print(f\" Reloaded {len(roi_masks)} ROIs, all shape = {target_xyz}\")"
      ],
      "metadata": {
        "id": "Fqh0YF7MxtJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## (3) Compute statistics and export for plotting later.\n",
        "\n",
        "# ---- Condition grouping (2 categories Faces / Letters) ----\n",
        "# To select only Faces / Letters, replace above with:\n",
        "coilist    = [1, 9]\n",
        "coi_labels = ['Faces','Letters']\n",
        "\n",
        "n_roi   = len(roi_masks)\n",
        "n_cond  = len(coilist)\n",
        "n_visit = len(betas)\n",
        "\n",
        "m  = np.zeros((n_visit, n_cond, n_roi))\n",
        "md = np.zeros((n_visit, n_cond, n_roi))\n",
        "sd = np.zeros((n_visit, n_cond, n_roi))\n",
        "se = np.zeros((n_visit, n_cond, n_roi))\n",
        "d  = {}\n",
        "\n",
        "# ---- Extract voxel statistics within each ROI per condition per visit ----\n",
        "for r, roihere in enumerate(roi_masks):\n",
        "    for c, cond_id in enumerate(coilist):\n",
        "        for v in range(n_visit):\n",
        "            modelmd   = betas[v]\n",
        "            stimorder = designs[v]['stimorder'].astype(int).ravel()\n",
        "            idx = np.where(stimorder == cond_id)[0]\n",
        "            if idx.size == 0:\n",
        "                raise ValueError(f\"No trials for cond_id {cond_id} at visit {v}\")\n",
        "\n",
        "            if modelmd.shape[-1] == len(stimorder):          # shape = (X,Y,Z,n_trials)\n",
        "                beta = modelmd[:, :, :, idx].mean(axis=3)\n",
        "            elif modelmd.shape[0] == len(stimorder):         # shape = (n_trials,X,Y,Z)\n",
        "                beta = modelmd[idx, ...].mean(axis=0)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected beta shape: {modelmd.shape}\")\n",
        "\n",
        "            beta, mask = align_xyz(beta, roihere.astype(bool),\n",
        "                                   tag=f\"visit={v}, roi={roi_names[r]}, cond={coi_labels[c]}\")\n",
        "            beta_flat = beta[mask]\n",
        "\n",
        "            m[v, c, r]  = beta_flat.mean()\n",
        "            md[v, c, r] = np.median(beta_flat)\n",
        "            sd[v, c, r] = beta_flat.std(ddof=0)\n",
        "            se[v, c, r] = sd[v, c, r] / np.sqrt(beta_flat.size)\n",
        "            d[(v, c, r)] = beta_flat\n",
        "\n",
        "print(\"âœ… All statistics extracted without axis mismatch.\")\n"
      ],
      "metadata": {
        "id": "uTLV7STJMv8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5**: Visualize trajectories of brain responses to faces and letters throughout first grade."
      ],
      "metadata": {
        "id": "WZeS_NcehWk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: Plot the mean betas for face and letter images within our ROIs at each of the 12 visits.  \n"
      ],
      "metadata": {
        "id": "6ew4Mt8rp5sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will**: (1) Create a data dictionary to store the statistics and (2) Plot them!"
      ],
      "metadata": {
        "id": "VuWJwDhfJqwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data dictionary for the statistics that will be plotted.\n",
        "\n",
        "all_roi_data = {}  # Initialize a dictionary to hold all data\n",
        "\n",
        "for r, roi_name in enumerate(roi_names):\n",
        "\n",
        "    # Create a dictionary for the current ROI's data\n",
        "    roi_data = {}\n",
        "\n",
        "    # Convert numpy array slices to pandas DataFrames and store them\n",
        "    roi_data['m'] = pd.DataFrame(m[:, :, r].tolist(), columns=coi_labels)\n",
        "    roi_data['sd'] = pd.DataFrame(sd[:, :, r].tolist(), columns=coi_labels)\n",
        "    roi_data['se'] = pd.DataFrame(se[:, :, r].tolist(), columns=coi_labels)\n",
        "    roi_data['md'] = pd.DataFrame(md[:, :, r].tolist(), columns=coi_labels)\n",
        "\n",
        "    # Add the current ROI's data dictionary to the main dictionary\n",
        "    all_roi_data[roi_name] = roi_data\n",
        "\n",
        "print(\"âœ… DataFrames created and stored in memory.\")"
      ],
      "metadata": {
        "id": "iCMEQwAOrghj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Plot!\n",
        "\n",
        "## -->> Simulate scenario with longitudinal sampling that's more or less dense.\n",
        "\n",
        "n_samples = 12 # You can change this number between 1 and 12 (e.g., 2, 6, 12)\n",
        "\n",
        "idx_visits = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "# Ensure n_samples is not greater than the number of available visits\n",
        "if n_samples > len(idx_visits):\n",
        "    n_samples = len(idx_visits)\n",
        "\n",
        "months = np.sort(np.array(random.sample(idx_visits, n_samples)))\n",
        "print(f\"Randomly selected {n_samples} months for plotting: {months}\")\n",
        "\n",
        "# Process and plot for each ROI using the data stored in the dictionary\n",
        "for roi_name, data in reversed(all_roi_data.items()):\n",
        "\n",
        "    print(f\"Processing {roi_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Retrieve dataframes from the dictionary\n",
        "        df_m = data['m']\n",
        "        df_md = data['md']\n",
        "        df_sd = data['sd']\n",
        "        df_se = data['se']\n",
        "\n",
        "        # Create plot using the randomly selected months\n",
        "        # Note: .iloc uses 0-based indexing, so we subtract 1 from the month numbers\n",
        "        plot_roi_results(df_m.iloc[months-1],\n",
        "                         df_md.iloc[months-1],\n",
        "                         df_sd.iloc[months-1],\n",
        "                         df_se.iloc[months-1],\n",
        "                         roi_name);\n",
        "\n",
        "    except Exception as e:\n",
        "       print(f\" \")\n",
        "\n",
        "print(\"\\nAll ROIs processed!\")\n"
      ],
      "metadata": {
        "id": "QXLPru5hrgrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE END**: Congratulations on your first analysis with dense longitudinal neuroimaging data! And, thank you!"
      ],
      "metadata": {
        "id": "hWgdgcSc9fLH"
      }
    }
  ]
}